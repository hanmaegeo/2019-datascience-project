{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peer Assessment\n",
    "\n",
    "### Where will you get your data from?\n",
    "\n",
    "Our personal bank transactions. We will collect data from every one of our bank transactions including someone else we know who is outside of our group that has had their credit card information stolen multiple times.\n",
    "\n",
    "### Project Deviations:\n",
    "\n",
    "One of the issues that we ran into is that when we collected the data from the individual with fraudulent transactions the bank had erased the transactions from the statement that we needed. Because of this, we needed to collect the data somewhere else. We decided to restructure our project around predicting load default. We got the data from https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)\n",
    "\n",
    "### How will you clean your data?\n",
    "\n",
    "We essentially will use GitHub as version control to push and pull with team members. Each team member will contribute to cleaning the data. We will do this by adding more columns such as time, state and whether or not it was online.  \n",
    "\n",
    "### Project Deviations:\n",
    "\n",
    "We needed to clean an ARFF file by converting each byte into strings and from each string into integers.\n",
    "\n",
    "### Where will you get your testing data from?\n",
    "\n",
    "We will just section off the data we have for testing our model. Since our data will probably be unbalanced we will make it test data with half fraud cases and half normal transaction cases so that our model can be more accurate. \n",
    "\n",
    "### Project Deviations:\n",
    "\n",
    "We now plan on not just training our model with equal fraudulent and equal normal loan applications, but will also train our models on different ratios of fraudulent to normal loan applications. \n",
    "\n",
    "### What algorithm will you use?\n",
    "\n",
    "We plan on using multiple algorithms, but we plan on using mainly the perceptron algorithm. We will use the perceptron algorithm attached to certain probabilities. \n",
    "\n",
    "### Project Deviations:\n",
    "\n",
    "We plan on using additional algorithms than just the perceptron algorithm. \n",
    "\n",
    "### Are there any variables that aren't being modelled, but will have an effect on the outcome?\n",
    "\n",
    "After looking at the data we won't have time stamps attached to our data.  \n",
    "\n",
    "### Project Deviations:\n",
    "\n",
    "Since we changed where we have acquired our data from we no longer have missing variables. We don't have variables that contribute to our loan default predictions that are missing. \n",
    "\n",
    "### What are the necessary things and what variables are just nice to have? \n",
    "\n",
    "We will need the transaction amount, whether or not it was online. Variables that aren't necessary are just noise that needs to be cleaned from our data.  \n",
    "\n",
    "### Project Deviations:\n",
    "\n",
    "We also added a gender column to answer some of our more curious questions.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
